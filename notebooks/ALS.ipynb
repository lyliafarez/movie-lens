{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df88afb-ac28-43f4-affb-ada38b19e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "#imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, count, mean, stddev, regexp_extract, avg, round, sum as _sum, floor,size,split,to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "#imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, count, mean, stddev, regexp_extract,avg, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "#initialisation de la session spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchETLPipelineEnhanced\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#chargement du csv movies\n",
    "movies = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/movie.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "#chargement du csv rating\n",
    "ratings = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/rating.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Nettoyage initial des données\n",
    "# Nettoyage des films\n",
    "movies = movies.dropna()  # Supprimer les lignes avec des valeurs manquantes\n",
    "movies = movies.dropDuplicates(['movieId'])  # Supprimer les doublons\n",
    "\n",
    "# Nettoyage des évaluations\n",
    "ratings = ratings.dropna()\n",
    "ratings = ratings.dropDuplicates(['userId', 'movieId'])  # Une évaluation par utilisateur/film\n",
    "\n",
    "ratings = ratings.withColumn('rating_date', to_date('timestamp')).withColumn('rating_year', year('timestamp')).withColumn('rating_month', month('timestamp')).withColumn('rating_day', dayofmonth('timestamp'))\n",
    "# Filtrer les évaluations extrêmes (trop basses ou trop hautes)\n",
    "ratings = ratings.filter((ratings.rating >= 0.5) & (ratings.rating <= 5.0))\n",
    "#ne garder que les films avec un minimum d'évaluations\n",
    "from pyspark.sql.functions import count\n",
    "movie_rating_counts = ratings.groupBy('movieId').agg(count('rating').alias('rating_count'))\n",
    "movies = movies.join(movie_rating_counts, 'movieId', 'left')\n",
    "movies = movies.filter(movies.rating_count >= 10)  \n",
    "#ne garder que les utilisateurs ayant évalué un minimum de films\n",
    "user_rating_counts = ratings.groupBy('userId').agg(count('rating').alias('user_rating_count'))\n",
    "ratings = ratings.join(user_rating_counts, 'userId', 'left')\n",
    "ratings = ratings.filter(ratings.user_rating_count >= 20)  # Seuil à ajuster\n",
    "\n",
    "# Ajout d'informations sur les films aux évaluations\n",
    "enriched_ratings = ratings.join(movies, \"movieId\", \"left\")\n",
    "\n",
    "# Calcul de la moyenne des notes par film\n",
    "movie_stats = ratings.groupBy(\"movieId\").agg(\n",
    "    avg(\"rating\").alias(\"avg_rating\"),\n",
    "    count(\"rating\").alias(\"num_ratings\")\n",
    ")\n",
    "\n",
    "# Calcul de l'activité des utilisateurs\n",
    "user_stats = ratings.groupBy(\"userId\").agg(\n",
    "    count(\"rating\").alias(\"user_total_ratings\"),\n",
    "    avg(\"rating\").alias(\"user_avg_rating\")\n",
    ")\n",
    "# Fusion des statistiques avec les données enrichies\n",
    "enriched_ratings = enriched_ratings.join(movie_stats, \"movieId\", \"left\")\n",
    "enriched_ratings = enriched_ratings.join(user_stats, \"userId\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe65ac9a-5587-463e-b881-70d669673153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/02 08:41:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#initialisation de la session spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchETLPipelineEnhanced\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324b15fb-2d53-487f-825b-66145bb50f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_dataframe(func, *args, error_msg=\"Erreur lors du chargement des données\", **kwargs):\n",
    "    \"\"\"Fonction utilitaire pour charger les dataframes en toute sécurité avec gestion des erreurs\"\"\"\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"{error_msg}: {str(e)}\")\n",
    "        import sys\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06298e43-285e-4e7b-a1bc-da7bbb0c3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Charger un très petit échantillon des films (limité à 100)\n",
    "movies_minimal = safe_load_dataframe(\n",
    "    spark.read.csv,\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/movie.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").limit(100000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a13fa64-2e95-4e00-ade8-dd4c8f92d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Charger un très petit échantillon des évaluations (limité à 1000)\n",
    "ratings_minimal = safe_load_dataframe(\n",
    "        spark.read.csv,\n",
    "        \"hdfs://namenode:9000/movie-lens/rawdata/rating.csv\", \n",
    "        header=True, inferSchema=True\n",
    ").limit(1000000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f21ce18-098f-455f-943f-84384982faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "'''#chargement du csv movies\n",
    "movies = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/movie.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce9de8f7-db56-48a9-b65e-e0332b702367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "'''#chargement du csv rating\n",
    "ratings = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/rating.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a6e5de-8b7b-4f6f-bd5f-c5f50e870161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage initial des données\n",
    "# Nettoyage des films\n",
    "movies = movies_minimal.dropna()  # Supprimer les lignes avec des valeurs manquantes\n",
    "movies = movies_minimal.dropDuplicates(['movieId'])  # Supprimer les doublons\n",
    "\n",
    "# Nettoyage des évaluations\n",
    "ratings = ratings_minimal.dropna()\n",
    "ratings = ratings_minimal.dropDuplicates(['userId', 'movieId'])  # Une évaluation par utilisateur/film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b780bb94-4f24-4b45-a6f8-ddfddf13218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8711e033-0177-496d-ace1-f000b894d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.withColumn('rating_date', to_date('timestamp')).withColumn('rating_year', year('timestamp')).withColumn('rating_month', month('timestamp')).withColumn('rating_day', dayofmonth('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf031074-903b-416e-a136-ca76427a0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les évaluations extrêmes (trop basses ou trop hautes)\n",
    "ratings = ratings.filter((ratings.rating >= 0.5) & (ratings.rating <= 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8e9418-57d6-427a-a154-ef802fe61952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne garder que les films avec un minimum d'évaluations\n",
    "from pyspark.sql.functions import count\n",
    "movie_rating_counts = ratings.groupBy('movieId').agg(count('rating').alias('rating_count'))\n",
    "movies = movies.join(movie_rating_counts, 'movieId', 'left')\n",
    "movies = movies.filter(movies.rating_count >= 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791f0a60-2003-4683-acd7-c9650150c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne garder que les utilisateurs ayant évalué un minimum de films\n",
    "user_rating_counts = ratings.groupBy('userId').agg(count('rating').alias('user_rating_count'))\n",
    "ratings = ratings.join(user_rating_counts, 'userId', 'left')\n",
    "ratings = ratings.filter(ratings.user_rating_count >= 20)  # Seuil à ajuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1ea3006-194d-4f9c-a3d3-b9d4ee70199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'informations sur les films aux évaluations\n",
    "enriched_ratings = ratings.join(movies, \"movieId\", \"left\")\n",
    "\n",
    "# Calcul de la moyenne des notes par film\n",
    "movie_stats = ratings.groupBy(\"movieId\").agg(\n",
    "    avg(\"rating\").alias(\"avg_rating\"),\n",
    "    count(\"rating\").alias(\"num_ratings\")\n",
    ")\n",
    "\n",
    "# Calcul de l'activité des utilisateurs\n",
    "user_stats = ratings.groupBy(\"userId\").agg(\n",
    "    count(\"rating\").alias(\"user_total_ratings\"),\n",
    "    avg(\"rating\").alias(\"user_avg_rating\")\n",
    ")\n",
    "# Fusion des statistiques avec les données enrichies\n",
    "enriched_ratings = enriched_ratings.join(movie_stats, \"movieId\", \"left\")\n",
    "enriched_ratings = enriched_ratings.join(user_stats, \"userId\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a769def4-bd50-43f6-a397-c4b3ef31c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import from_json, col, struct, rand\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, TimestampType\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e124fc-09a8-4848-8b3b-798d54d456b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9f0708e-73f4-44e7-a3d7-d272f80fd1e3",
   "metadata": {},
   "source": [
    "# 1. Préparation des données pour le modèle ALS\n",
    "# Sélection des colonnes nécessaires pour l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39a22cf5-9599-4c39-9d41-66fa6d3254d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel mais recommandé : réordonner de manière aléatoire pour split\n",
    "ratings_randomized = enriched_ratings.orderBy(rand())\n",
    "\n",
    "# Split train/test\n",
    "train, test = ratings_randomized.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5abf614-11a2-43c9-b9d2-09149cab37a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entrainement du model\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    rank=20,\n",
    "    maxIter=10, \n",
    "    regParam=0.1,\n",
    "    coldStartStrategy=\"drop\",  # Pour éviter NaN lors des prédictions\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c39048fb-52c0-4104-8ccd-1974c98e610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 08:43:24 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0efe96c8-4e7e-4632-9476-944909b9541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 195:>                                                        (0 + 8) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE du modèle ALS sur le test set : 0.7546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# evaluation du model\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE du modèle ALS sur le test set : {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709d0e7-4e30-4a58-87af-665d319f4b30",
   "metadata": {},
   "source": [
    "## Sauvegarde HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55f4dcea-312d-4b65-b91a-2ad478d12ebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moverwrite()\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:9000/movie-lens/models/als_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/util.py:273\u001b[0m, in \u001b[0;36mJavaMLWritable.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaMLWriter:\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an MLWriter instance for this ML instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJavaMLWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/util.py:206\u001b[0m, in \u001b[0;36mJavaMLWriter.__init__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLWritable\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28msuper\u001b[39m(JavaMLWriter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 206\u001b[0m     _java_obj \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite \u001b[38;5;241m=\u001b[39m _java_obj\u001b[38;5;241m.\u001b[39mwrite()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py:267\u001b[0m, in \u001b[0;36mJavaParams._to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_java\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Transfer this instance's Params to the wrapped Java object, and return the Java object.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Used for ML persistence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m        Java object equivalent to this instance.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transfer_params_to_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py:174\u001b[0m, in \u001b[0;36mJavaParams._transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mset(pair)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasDefault(param):\n\u001b[0;32m--> 174\u001b[0m         pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_java_param_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaultParamMap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m         pair_defaults\u001b[38;5;241m.\u001b[39mappend(pair)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair_defaults) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py:158\u001b[0m, in \u001b[0;36mJavaParams._make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    157\u001b[0m param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolveParam(param)\n\u001b[0;32m--> 158\u001b[0m java_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetParam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m java_value \u001b[38;5;241m=\u001b[39m _py2java(sc, value)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_param\u001b[38;5;241m.\u001b[39mw(java_value)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save(\"hdfs://namenode:9000/movie-lens/models/als_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
