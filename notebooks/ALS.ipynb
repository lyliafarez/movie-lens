{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df88afb-ac28-43f4-affb-ada38b19e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 15:37:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "#imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, count, mean, stddev, regexp_extract, avg, round, sum as _sum, floor,size,split,to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#initialisation de la session spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchETLPipelineEnhanced\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#chargement du csv movies\n",
    "movies = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/movie.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "#chargement du csv rating\n",
    "ratings = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/rating.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Nettoyage initial des données\n",
    "# Nettoyage des films\n",
    "movies = movies.dropna()  # Supprimer les lignes avec des valeurs manquantes\n",
    "movies = movies.dropDuplicates(['movieId'])  # Supprimer les doublons\n",
    "\n",
    "# Nettoyage des évaluations\n",
    "ratings = ratings.dropna()\n",
    "ratings = ratings.dropDuplicates(['userId', 'movieId'])  # Une évaluation par utilisateur/film\n",
    "\n",
    "ratings = ratings.withColumn('rating_date', to_date('timestamp')).withColumn('rating_year', year('timestamp')).withColumn('rating_month', month('timestamp')).withColumn('rating_day', dayofmonth('timestamp'))\n",
    "# Filtrer les évaluations extrêmes (trop basses ou trop hautes)\n",
    "ratings = ratings.filter((ratings.rating >= 0.5) & (ratings.rating <= 5.0))\n",
    "#ne garder que les films avec un minimum d'évaluations\n",
    "from pyspark.sql.functions import count\n",
    "movie_rating_counts = ratings.groupBy('movieId').agg(count('rating').alias('rating_count'))\n",
    "movies = movies.join(movie_rating_counts, 'movieId', 'left')\n",
    "movies = movies.filter(movies.rating_count >= 10)  \n",
    "#ne garder que les utilisateurs ayant évalué un minimum de films\n",
    "user_rating_counts = ratings.groupBy('userId').agg(count('rating').alias('user_rating_count'))\n",
    "ratings = ratings.join(user_rating_counts, 'userId', 'left')\n",
    "ratings = ratings.filter(ratings.user_rating_count >= 20)  # Seuil à ajuster\n",
    "\n",
    "# Ajout d'informations sur les films aux évaluations\n",
    "enriched_ratings = ratings.join(movies, \"movieId\", \"left\")\n",
    "\n",
    "# Calcul de la moyenne des notes par film\n",
    "movie_stats = ratings.groupBy(\"movieId\").agg(\n",
    "    avg(\"rating\").alias(\"avg_rating\"),\n",
    "    count(\"rating\").alias(\"num_ratings\")\n",
    ")\n",
    "\n",
    "# Calcul de l'activité des utilisateurs\n",
    "user_stats = ratings.groupBy(\"userId\").agg(\n",
    "    count(\"rating\").alias(\"user_total_ratings\"),\n",
    "    avg(\"rating\").alias(\"user_avg_rating\")\n",
    ")\n",
    "# Fusion des statistiques avec les données enrichies\n",
    "enriched_ratings = enriched_ratings.join(movie_stats, \"movieId\", \"left\")\n",
    "enriched_ratings = enriched_ratings.join(user_stats, \"userId\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd44df4-6ef6-4658-9987-7ec5cf1a93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, count, mean, stddev, regexp_extract,avg, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe65ac9a-5587-463e-b881-70d669673153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation de la session spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BatchETLPipelineEnhanced\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324b15fb-2d53-487f-825b-66145bb50f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_dataframe(func, *args, error_msg=\"Erreur lors du chargement des données\", **kwargs):\n",
    "    \"\"\"Fonction utilitaire pour charger les dataframes en toute sécurité avec gestion des erreurs\"\"\"\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"{error_msg}: {str(e)}\")\n",
    "        import sys\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06298e43-285e-4e7b-a1bc-da7bbb0c3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Charger un très petit échantillon des films (limité à 100)\n",
    "movies_minimal = safe_load_dataframe(\n",
    "    spark.read.csv,\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/movie.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").limit(100000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a13fa64-2e95-4e00-ade8-dd4c8f92d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Charger un très petit échantillon des évaluations (limité à 1000)\n",
    "ratings_minimal = safe_load_dataframe(\n",
    "        spark.read.csv,\n",
    "        \"hdfs://namenode:9000/movie-lens/rawdata/rating.csv\", \n",
    "        header=True, inferSchema=True\n",
    ").limit(1000000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80efad8-7379-450c-846b-63f4ab7c00ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d2731-ccfd-47d4-a8cd-ec1d64b0c055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029be6bc-4e58-417c-85bc-94a7f34b5a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf0471-a64e-4c71-b942-afda95bdc39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f21ce18-098f-455f-943f-84384982faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "'''#chargement du csv movies\n",
    "movies = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/movie.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce9de8f7-db56-48a9-b65e-e0332b702367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "'''#chargement du csv rating\n",
    "ratings = spark.read.csv(\n",
    "    \"hdfs://namenode:9000/movie-lens/rawdata/rating.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a6e5de-8b7b-4f6f-bd5f-c5f50e870161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage initial des données\n",
    "# Nettoyage des films\n",
    "movies = movies_minimal.dropna()  # Supprimer les lignes avec des valeurs manquantes\n",
    "movies = movies_minimal.dropDuplicates(['movieId'])  # Supprimer les doublons\n",
    "\n",
    "# Nettoyage des évaluations\n",
    "ratings = ratings_minimal.dropna()\n",
    "ratings = ratings_minimal.dropDuplicates(['userId', 'movieId'])  # Une évaluation par utilisateur/film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b780bb94-4f24-4b45-a6f8-ddfddf13218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8711e033-0177-496d-ace1-f000b894d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.withColumn('rating_date', to_date('timestamp')).withColumn('rating_year', year('timestamp')).withColumn('rating_month', month('timestamp')).withColumn('rating_day', dayofmonth('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf031074-903b-416e-a136-ca76427a0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les évaluations extrêmes (trop basses ou trop hautes)\n",
    "ratings = ratings.filter((ratings.rating >= 0.5) & (ratings.rating <= 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8e9418-57d6-427a-a154-ef802fe61952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne garder que les films avec un minimum d'évaluations\n",
    "from pyspark.sql.functions import count\n",
    "movie_rating_counts = ratings.groupBy('movieId').agg(count('rating').alias('rating_count'))\n",
    "movies = movies.join(movie_rating_counts, 'movieId', 'left')\n",
    "movies = movies.filter(movies.rating_count >= 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791f0a60-2003-4683-acd7-c9650150c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne garder que les utilisateurs ayant évalué un minimum de films\n",
    "user_rating_counts = ratings.groupBy('userId').agg(count('rating').alias('user_rating_count'))\n",
    "ratings = ratings.join(user_rating_counts, 'userId', 'left')\n",
    "ratings = ratings.filter(ratings.user_rating_count >= 20)  # Seuil à ajuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ea3006-194d-4f9c-a3d3-b9d4ee70199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'informations sur les films aux évaluations\n",
    "enriched_ratings = ratings.join(movies, \"movieId\", \"left\")\n",
    "\n",
    "# Calcul de la moyenne des notes par film\n",
    "movie_stats = ratings.groupBy(\"movieId\").agg(\n",
    "    avg(\"rating\").alias(\"avg_rating\"),\n",
    "    count(\"rating\").alias(\"num_ratings\")\n",
    ")\n",
    "\n",
    "# Calcul de l'activité des utilisateurs\n",
    "user_stats = ratings.groupBy(\"userId\").agg(\n",
    "    count(\"rating\").alias(\"user_total_ratings\"),\n",
    "    avg(\"rating\").alias(\"user_avg_rating\")\n",
    ")\n",
    "# Fusion des statistiques avec les données enrichies\n",
    "enriched_ratings = enriched_ratings.join(movie_stats, \"movieId\", \"left\")\n",
    "enriched_ratings = enriched_ratings.join(user_stats, \"userId\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a769def4-bd50-43f6-a397-c4b3ef31c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import from_json, col, struct, rand\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, TimestampType\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e124fc-09a8-4848-8b3b-798d54d456b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9f0708e-73f4-44e7-a3d7-d272f80fd1e3",
   "metadata": {},
   "source": [
    "# 1. Préparation des données pour le modèle ALS\n",
    "# Sélection des colonnes nécessaires pour l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a22cf5-9599-4c39-9d41-66fa6d3254d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel mais recommandé : réordonner de manière aléatoire pour split\n",
    "ratings_randomized = ratings.orderBy(rand())\n",
    "\n",
    "# Split train/test\n",
    "train, test = ratings_randomized.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5abf614-11a2-43c9-b9d2-09149cab37a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entrainement du model\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    rank=20,\n",
    "    maxIter=10, \n",
    "    regParam=0.1,\n",
    "    coldStartStrategy=\"drop\",  # Pour éviter NaN lors des prédictions\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39048fb-52c0-4104-8ccd-1974c98e610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0efe96c8-4e7e-4632-9476-944909b9541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 215:>                                                        (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE du modèle ALS sur le test set : 0.7516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# evaluation du model\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE du modèle ALS sur le test set : {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709d0e7-4e30-4a58-87af-665d319f4b30",
   "metadata": {},
   "source": [
    "## Sauvegarde HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55f4dcea-312d-4b65-b91a-2ad478d12ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save(\"hdfs://namenode:9000/movie-lens/models/als_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
