{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831046a2-6487-4adb-b40e-95c0a5aca32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-4.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading numpy-2.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pymongo-4.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Installing collected packages: numpy, dnspython, pymongo\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pymongo]m2/3\u001b[0m [pymongo]n]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dnspython-2.7.0 numpy-2.2.5 pymongo-4.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aab9f72-c188-43f3-8732-9727879ef060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType\n",
    "import signal\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc29d52-b509-4056-a1b3-067d8edd1201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:15:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/02 10:15:08 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/02 10:15:16 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10019 milliseconds\n",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n",
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n",
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n",
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n",
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n",
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:15:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/02 10:15:56 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (namenode executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/02 10:15:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 0, Input rows: 0, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:15:56 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 40133 milliseconds\n",
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 1, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 1, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 1, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 1, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 1, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 1, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:16:28 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/02 10:16:28 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (namenode executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/02 10:16:28 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n",
      "25/05/02 10:16:29 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 32709 milliseconds\n",
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 2, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 2, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 2, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 2, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 2, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 2, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:17:01 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/02 10:17:01 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (namenode executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/02 10:17:01 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job\n",
      "25/05/02 10:17:01 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 32224 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 2, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 3, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 3, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 3, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 3, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 3, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 3, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:17:33 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 7)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/02 10:17:33 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7) (namenode executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/02 10:17:33 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job\n",
      "25/05/02 10:17:33 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 31711 milliseconds\n",
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 4, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 4, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 4, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 4, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 4, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 4, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:18:05 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 9)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/02 10:18:05 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 9) (namenode executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/02 10:18:05 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n",
      "25/05/02 10:18:05 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 32323 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 5, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 5, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 5, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 5, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 5, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 5, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 5, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:18:37 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 11)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/02 10:18:38 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 11) (namenode executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/02 10:18:38 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job\n",
      "25/05/02 10:18:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 32789 milliseconds\n",
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 6, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 6, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 6, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 6, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 6, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 6, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:19:10 ERROR Executor: Exception in task 0.0 in stage 13.0 (TID 13)\n",
      "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/02 10:19:10 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 13) (namenode executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=host.docker.internal:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
      "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
      "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/05/02 10:19:10 ERROR TaskSetManager: Task 0 in stage 13.0 failed 1 times; aborting job\n",
      "25/05/02 10:19:11 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 32468 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 7, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 7, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 7, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 7, Input rows: 1, Processed: 0.0 rows/sec\n",
      "Batch ID: 7, Input rows: 1, Processed: 0.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:19:35 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 24806 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 8, Input rows: 33, Processed: 1.3 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 9, Input rows: 25, Processed: 7.8 rows/sec\n",
      "Batch ID: 9, Input rows: 25, Processed: 7.8 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 10:19:51 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 11205 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 10, Input rows: 5, Processed: 0.4 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 11, Input rows: 11, Processed: 5.5 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 11, Input rows: 11, Processed: 5.5 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 12, Input rows: 11, Processed: 2.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 12, Input rows: 11, Processed: 2.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 13, Input rows: 11, Processed: 3.7 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 13, Input rows: 11, Processed: 3.7 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 14, Input rows: 9, Processed: 2.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 14, Input rows: 9, Processed: 2.9 rows/sec\n",
      "Batch ID: 15, Input rows: 11, Processed: 3.5 rows/sec\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with proper configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerSparkToLocalMongo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://host.docker.internal:27017/movie_lens.recommendations\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define schema for incoming Kafka messages\n",
    "rating_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType()),  # Changed from user_id\n",
    "    StructField(\"movieId\", IntegerType()),  # Changed from movie_id\n",
    "    StructField(\"rating\", FloatType()),\n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Function to process each batch of data\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if not batch_df.isEmpty():\n",
    "        try:\n",
    "            # Add processing metadata\n",
    "            result_df = batch_df.withColumn(\"processing_time\", F.current_timestamp()) \\\n",
    "                              .withColumn(\"batch_id\", F.lit(batch_id))\n",
    "            \n",
    "            # Write to MongoDB\n",
    "            (result_df.write\n",
    "                .format(\"mongo\")\n",
    "                .mode(\"append\")\n",
    "                .option(\"database\", \"movie_lens\")\n",
    "                .option(\"collection\", \"recommendations\")\n",
    "                .save())\n",
    "            \n",
    "            print(f\"Successfully processed batch {batch_id} with {batch_df.count()} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_id}: {str(e)}\", file=sys.stderr)\n",
    "\n",
    "# Create Kafka source stream\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"namenode:9092\") \\\n",
    "    .option(\"subscribe\", \"movie_rating\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", \"1000\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON data from Kafka\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "                .select(F.from_json(F.col(\"value\"), rating_schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "\n",
    "# Start the streaming query\n",
    "query = processed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"file:///tmp/checkpoint_movies\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Graceful shutdown handler\n",
    "def handle_shutdown(signum, frame):\n",
    "    print(\"\\nShutting down gracefully...\")\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_shutdown)\n",
    "signal.signal(signal.SIGTERM, handle_shutdown)\n",
    "\n",
    "# Monitoring loop\n",
    "try:\n",
    "    while query.isActive:\n",
    "        progress = query.lastProgress\n",
    "        if progress:\n",
    "            print(f\"Batch ID: {progress['batchId']}, \"\n",
    "                  f\"Input rows: {progress['numInputRows']}, \"\n",
    "                  f\"Processed: {progress['processedRowsPerSecond']:.1f} rows/sec\")\n",
    "        time.sleep(5)\n",
    "except Exception as e:\n",
    "    print(f\"Streaming query failed: {str(e)}\", file=sys.stderr)\n",
    "    handle_shutdown(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13468dbe-9130-4395-93fb-7ec9a4d15206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
