{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831046a2-6487-4adb-b40e-95c0a5aca32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.12.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.7.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83723d0e-412a-4281-8b3d-0c290073badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType\n",
    "import sys\n",
    "import time\n",
    "import signal\n",
    "from pyspark.ml.recommendation import ALSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "218a2221-f1c0-4192-acf7-942a8a401398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with proper configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerSparkToLocalMongo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://host.docker.internal:27017/movie_lens.recommendations\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0c0488d-4fa8-4389-b4b3-0b28afed7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ALS model\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ALS model\n",
    "model_path = \"hdfs://namenode:9000/movie-lens/models/als_model\"\n",
    "try:\n",
    "    model = ALSModel.load(model_path)\n",
    "    print(\"Successfully loaded ALS model\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load ALS model: {str(e)}\", file=sys.stderr)\n",
    "    spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "# Define schema for incoming Kafka messages\n",
    "rating_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType()),\n",
    "    StructField(\"movieId\", IntegerType()),\n",
    "    StructField(\"rating\", FloatType()),\n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74fce45c-63d3-4322-b504-2a9cae1301f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(user_ids_df):\n",
    "    # Get distinct user IDs (assumes small batch, fast op)\n",
    "    distinct_users = user_ids_df.select(\"userId\").distinct()\n",
    "\n",
    "    # Generate top 5 recommendations\n",
    "    recommendations = model.recommendForUserSubset(distinct_users, 5)\n",
    "\n",
    "    # Explode array of recommendations into rows\n",
    "    exploded_recs = recommendations.select(\n",
    "        \"userId\",\n",
    "        F.explode(\"recommendations\").alias(\"recommendation\")\n",
    "    ).select(\n",
    "        \"userId\",\n",
    "        F.col(\"recommendation.movieId\").alias(\"movieId\"),\n",
    "        F.col(\"recommendation.rating\").alias(\"predictedRating\")\n",
    "    )\n",
    "\n",
    "    return exploded_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e80b980-c1c1-4b2b-8d79-5140537a9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    if not batch_df.isEmpty():\n",
    "        try:\n",
    "            # Generate recommendations for users in this batch\n",
    "            recommendations_df = generate_recommendations(batch_df)\n",
    "\n",
    "            # Add metadata columns\n",
    "            result_df = recommendations_df.withColumn(\"processing_time\", F.current_timestamp()) \\\n",
    "                                          .withColumn(\"batch_id\", F.lit(batch_id))\n",
    "\n",
    "            # Write to MongoDB\n",
    "            (result_df.write\n",
    "                .format(\"mongo\")\n",
    "                .mode(\"append\")\n",
    "                .option(\"database\", \"movie_lens\")\n",
    "                .option(\"collection\", \"recommendations\")\n",
    "                .save())\n",
    "\n",
    "            print(f\"Batch {batch_id} processed: {result_df.count()} recommendations saved.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_id}: {str(e)}\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e38658ee-f11f-453b-b476-0a5899394774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kafka source stream\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"namenode:9092\") \\\n",
    "    .option(\"subscribe\", \"movie_rating\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", \"1000\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51188e0c-f8c7-4e81-ad56-887ac8d1b9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 13:26:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/02 13:26:00 WARN StreamingQueryManager: Stopping existing streaming query [id=50a8cf76-ef4a-4ccc-ad2a-4b81e576d59c, runId=633ab1f8-bd4e-42d8-9bfd-455b02d85e95], as a new run is being started.\n",
      "25/05/02 13:26:00 WARN OffsetSeqMetadata: Updating the value of conf 'spark.sql.shuffle.partitions' in current session from '200' to '4'.\n",
      "25/05/02 13:26:00 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "# Parse the JSON data from Kafka\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "                .select(F.from_json(F.col(\"value\"), rating_schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "\n",
    "# Start the streaming query\n",
    "query = processed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_movies\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f07e56cc-dd37-4786-94ed-d11a32b3bba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.handle_shutdown(signum, frame)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graceful shutdown handler\n",
    "def handle_shutdown(signum, frame):\n",
    "    print(\"\\nShutting down gracefully...\")\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_shutdown)\n",
    "signal.signal(signal.SIGTERM, handle_shutdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04586c7d-8203-4250-b985-7a42499a8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 118, Input rows: 0, Processed: 0.0 rows/sec\n",
      "Batch ID: 118, Input rows: 0, Processed: 0.0 rows/sec\n",
      "Batch ID: 118, Input rows: 3, Processed: 1.3 rows/sec\n",
      "Batch ID: 118, Input rows: 3, Processed: 1.3 rows/sec\n",
      "Batch ID: 119, Input rows: 11, Processed: 5.5 rows/sec\n",
      "Batch ID: 119, Input rows: 11, Processed: 5.5 rows/sec\n",
      "Batch ID: 120, Input rows: 11, Processed: 4.0 rows/sec\n",
      "Batch ID: 120, Input rows: 11, Processed: 4.0 rows/sec\n",
      "Batch ID: 121, Input rows: 11, Processed: 3.0 rows/sec\n",
      "Batch ID: 121, Input rows: 11, Processed: 3.0 rows/sec\n",
      "Batch ID: 122, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 122, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 123, Input rows: 11, Processed: 4.7 rows/sec\n",
      "Batch ID: 123, Input rows: 11, Processed: 4.7 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 124, Input rows: 11, Processed: 4.7 rows/sec\n",
      "Batch ID: 124, Input rows: 11, Processed: 4.7 rows/sec\n",
      "Batch ID: 125, Input rows: 9, Processed: 5.0 rows/sec\n",
      "Batch ID: 125, Input rows: 9, Processed: 5.0 rows/sec\n",
      "Batch ID: 126, Input rows: 11, Processed: 5.0 rows/sec\n",
      "Batch ID: 126, Input rows: 11, Processed: 5.0 rows/sec\n",
      "Batch ID: 127, Input rows: 11, Processed: 4.0 rows/sec\n",
      "Batch ID: 127, Input rows: 11, Processed: 4.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 128, Input rows: 11, Processed: 4.8 rows/sec\n",
      "Batch ID: 128, Input rows: 11, Processed: 4.8 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 129, Input rows: 11, Processed: 3.3 rows/sec\n",
      "Batch ID: 129, Input rows: 11, Processed: 3.3 rows/sec\n",
      "Batch ID: 130, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 130, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 131, Input rows: 11, Processed: 3.1 rows/sec\n",
      "Batch ID: 131, Input rows: 11, Processed: 3.1 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 132, Input rows: 11, Processed: 3.1 rows/sec\n",
      "Batch ID: 132, Input rows: 11, Processed: 3.1 rows/sec\n",
      "Batch ID: 133, Input rows: 11, Processed: 4.9 rows/sec\n",
      "Batch ID: 133, Input rows: 11, Processed: 4.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 134, Input rows: 11, Processed: 2.8 rows/sec\n",
      "Batch ID: 134, Input rows: 11, Processed: 2.8 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 134, Input rows: 11, Processed: 2.8 rows/sec\n",
      "Batch ID: 135, Input rows: 11, Processed: 1.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 136, Input rows: 11, Processed: 2.7 rows/sec\n",
      "Batch ID: 136, Input rows: 11, Processed: 2.7 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 137, Input rows: 11, Processed: 3.8 rows/sec\n",
      "Batch ID: 137, Input rows: 11, Processed: 3.8 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 138, Input rows: 11, Processed: 3.5 rows/sec\n",
      "Batch ID: 138, Input rows: 11, Processed: 3.5 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 139, Input rows: 9, Processed: 3.4 rows/sec\n",
      "Batch ID: 139, Input rows: 9, Processed: 3.4 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 140, Input rows: 11, Processed: 3.9 rows/sec\n",
      "Batch ID: 140, Input rows: 11, Processed: 3.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 141, Input rows: 11, Processed: 2.8 rows/sec\n",
      "Batch ID: 141, Input rows: 11, Processed: 2.8 rows/sec\n",
      "Batch ID: 142, Input rows: 11, Processed: 5.5 rows/sec\n",
      "Batch ID: 142, Input rows: 11, Processed: 5.5 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 142, Input rows: 11, Processed: 5.5 rows/sec\n",
      "Batch ID: 143, Input rows: 11, Processed: 2.0 rows/sec\n",
      "Batch ID: 144, Input rows: 11, Processed: 3.0 rows/sec\n",
      "Batch ID: 144, Input rows: 11, Processed: 3.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 145, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 145, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 146, Input rows: 11, Processed: 3.5 rows/sec\n",
      "Batch ID: 146, Input rows: 11, Processed: 3.5 rows/sec\n",
      "Batch ID: 147, Input rows: 11, Processed: 4.0 rows/sec\n",
      "Batch ID: 147, Input rows: 11, Processed: 4.0 rows/sec\n",
      "Batch ID: 148, Input rows: 11, Processed: 3.1 rows/sec\n",
      "Batch ID: 148, Input rows: 11, Processed: 3.1 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 149, Input rows: 11, Processed: 2.9 rows/sec\n",
      "Batch ID: 149, Input rows: 11, Processed: 2.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 150, Input rows: 9, Processed: 3.2 rows/sec\n",
      "Batch ID: 150, Input rows: 9, Processed: 3.2 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 151, Input rows: 11, Processed: 3.4 rows/sec\n",
      "Batch ID: 151, Input rows: 11, Processed: 3.4 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 152, Input rows: 11, Processed: 3.7 rows/sec\n",
      "Batch ID: 152, Input rows: 11, Processed: 3.7 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 153, Input rows: 11, Processed: 3.6 rows/sec\n",
      "Batch ID: 153, Input rows: 11, Processed: 3.6 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 154, Input rows: 11, Processed: 5.5 rows/sec\n",
      "Batch ID: 154, Input rows: 11, Processed: 5.5 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 155, Input rows: 11, Processed: 2.4 rows/sec\n",
      "Batch ID: 155, Input rows: 11, Processed: 2.4 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 156, Input rows: 11, Processed: 4.5 rows/sec\n",
      "Batch ID: 156, Input rows: 11, Processed: 4.5 rows/sec\n",
      "Batch ID: 157, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 157, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 158, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 158, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 159, Input rows: 9, Processed: 3.2 rows/sec\n",
      "Batch ID: 159, Input rows: 9, Processed: 3.2 rows/sec\n",
      "Batch ID: 160, Input rows: 11, Processed: 3.1 rows/sec\n",
      "Batch ID: 160, Input rows: 11, Processed: 3.1 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 161, Input rows: 11, Processed: 3.9 rows/sec\n",
      "Batch ID: 161, Input rows: 11, Processed: 3.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 162, Input rows: 11, Processed: 3.4 rows/sec\n",
      "Batch ID: 162, Input rows: 11, Processed: 3.4 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 163, Input rows: 11, Processed: 3.1 rows/sec\n",
      "Batch ID: 163, Input rows: 11, Processed: 3.1 rows/sec\n",
      "Batch ID: 164, Input rows: 11, Processed: 4.2 rows/sec\n",
      "Batch ID: 164, Input rows: 11, Processed: 4.2 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 165, Input rows: 11, Processed: 2.5 rows/sec\n",
      "Batch ID: 165, Input rows: 11, Processed: 2.5 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 166, Input rows: 11, Processed: 2.8 rows/sec\n",
      "Batch ID: 166, Input rows: 11, Processed: 2.8 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 167, Input rows: 11, Processed: 3.3 rows/sec\n",
      "Batch ID: 167, Input rows: 11, Processed: 3.3 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 168, Input rows: 11, Processed: 2.6 rows/sec\n",
      "Batch ID: 168, Input rows: 11, Processed: 2.6 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 169, Input rows: 9, Processed: 2.2 rows/sec\n",
      "Batch ID: 169, Input rows: 9, Processed: 2.2 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m progress:\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumInputRows\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessedRowsPerSecond\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows/sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming query failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Monitoring loop\n",
    "try:\n",
    "    while query.isActive:\n",
    "        progress = query.lastProgress\n",
    "        if progress:\n",
    "            print(f\"Batch ID: {progress['batchId']}, \"\n",
    "                  f\"Input rows: {progress['numInputRows']}, \"\n",
    "                  f\"Processed: {progress['processedRowsPerSecond']:.1f} rows/sec\")\n",
    "        time.sleep(5)\n",
    "except Exception as e:\n",
    "    print(f\"Streaming query failed: {str(e)}\", file=sys.stderr)\n",
    "    handle_shutdown(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157328d7-f17d-458f-ab90-739b2e23c2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
