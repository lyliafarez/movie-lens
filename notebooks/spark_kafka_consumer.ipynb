{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831046a2-6487-4adb-b40e-95c0a5aca32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-4.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading pymongo-4.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Installing collected packages: dnspython, pymongo\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pymongo]m1/2\u001b[0m [pymongo]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dnspython-2.7.0 pymongo-4.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aab9f72-c188-43f3-8732-9727879ef060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType\n",
    "import signal\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cc29d52-b509-4056-a1b3-067d8edd1201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/02 09:56:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/02 09:56:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/02 09:56:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 19, Input rows: 13, Processed: 3.7 rows/sec\n",
      "Batch ID: 20, Input rows: 3, Processed: 1.6 rows/sec\n",
      "Batch ID: 21, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 21, Input rows: 11, Processed: 4.4 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 22, Input rows: 11, Processed: 4.2 rows/sec\n",
      "Batch ID: 22, Input rows: 11, Processed: 4.2 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 23, Input rows: 11, Processed: 6.7 rows/sec\n",
      "Batch ID: 23, Input rows: 11, Processed: 6.7 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 24, Input rows: 11, Processed: 5.9 rows/sec\n",
      "Batch ID: 24, Input rows: 11, Processed: 5.9 rows/sec\n",
      "Batch ID: 25, Input rows: 11, Processed: 3.9 rows/sec\n",
      "Batch ID: 25, Input rows: 11, Processed: 3.9 rows/sec\n",
      "Batch ID: 26, Input rows: 11, Processed: 4.0 rows/sec\n",
      "Batch ID: 26, Input rows: 11, Processed: 4.0 rows/sec\n",
      "Batch ID: 27, Input rows: 11, Processed: 3.9 rows/sec\n",
      "Batch ID: 27, Input rows: 11, Processed: 3.9 rows/sec\n",
      "Batch ID: 28, Input rows: 11, Processed: 4.8 rows/sec\n",
      "Batch ID: 28, Input rows: 11, Processed: 4.8 rows/sec\n",
      "Batch ID: 29, Input rows: 11, Processed: 4.9 rows/sec\n",
      "Batch ID: 29, Input rows: 11, Processed: 4.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 30, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 30, Input rows: 11, Processed: 4.4 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 31, Input rows: 11, Processed: 4.5 rows/sec\n",
      "Batch ID: 31, Input rows: 11, Processed: 4.5 rows/sec\n",
      "Batch ID: 32, Input rows: 9, Processed: 4.0 rows/sec\n",
      "Batch ID: 32, Input rows: 9, Processed: 4.0 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 33, Input rows: 11, Processed: 4.8 rows/sec\n",
      "Batch ID: 33, Input rows: 11, Processed: 4.8 rows/sec\n",
      "Batch ID: 34, Input rows: 11, Processed: 4.9 rows/sec\n",
      "Batch ID: 34, Input rows: 11, Processed: 4.9 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 35, Input rows: 11, Processed: 6.2 rows/sec\n",
      "Batch ID: 35, Input rows: 11, Processed: 6.2 rows/sec\n",
      "Batch ID: 36, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 36, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 37, Input rows: 11, Processed: 5.3 rows/sec\n",
      "Batch ID: 37, Input rows: 11, Processed: 5.3 rows/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch ID: 38, Input rows: 11, Processed: 4.9 rows/sec\n",
      "Batch ID: 38, Input rows: 11, Processed: 4.9 rows/sec\n",
      "Batch ID: 39, Input rows: 11, Processed: 4.8 rows/sec\n",
      "Batch ID: 39, Input rows: 11, Processed: 4.8 rows/sec\n",
      "Batch ID: 40, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 40, Input rows: 11, Processed: 4.4 rows/sec\n",
      "Batch ID: 41, Input rows: 11, Processed: 4.1 rows/sec\n",
      "\n",
      "Shutting down gracefully...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with proper configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DockerSparkToLocalMongo\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\"\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://host.docker.internal:27017/movie_lens.recommendations\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define schema for incoming Kafka messages\n",
    "rating_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType()),  # Changed from user_id\n",
    "    StructField(\"movieId\", IntegerType()),  # Changed from movie_id\n",
    "    StructField(\"rating\", FloatType()),\n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Function to process each batch of data\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if not batch_df.isEmpty():\n",
    "        try:\n",
    "            # Add processing metadata\n",
    "            result_df = batch_df.withColumn(\"processing_time\", F.current_timestamp()) \\\n",
    "                              .withColumn(\"batch_id\", F.lit(batch_id))\n",
    "            \n",
    "            # Write to MongoDB\n",
    "            (result_df.write\n",
    "                .format(\"mongo\")\n",
    "                .mode(\"append\")\n",
    "                .option(\"database\", \"movie_lens\")\n",
    "                .option(\"collection\", \"recommendations\")\n",
    "                .save())\n",
    "            \n",
    "            print(f\"Successfully processed batch {batch_id} with {batch_df.count()} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_id}: {str(e)}\", file=sys.stderr)\n",
    "\n",
    "# Create Kafka source stream\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"namenode:9092\") \\\n",
    "    .option(\"subscribe\", \"movie_rating\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", \"1000\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON data from Kafka\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "                .select(F.from_json(F.col(\"value\"), rating_schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "\n",
    "# Start the streaming query\n",
    "query = processed_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_movies\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Graceful shutdown handler\n",
    "def handle_shutdown(signum, frame):\n",
    "    print(\"\\nShutting down gracefully...\")\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_shutdown)\n",
    "signal.signal(signal.SIGTERM, handle_shutdown)\n",
    "\n",
    "# Monitoring loop\n",
    "try:\n",
    "    while query.isActive:\n",
    "        progress = query.lastProgress\n",
    "        if progress:\n",
    "            print(f\"Batch ID: {progress['batchId']}, \"\n",
    "                  f\"Input rows: {progress['numInputRows']}, \"\n",
    "                  f\"Processed: {progress['processedRowsPerSecond']:.1f} rows/sec\")\n",
    "        time.sleep(5)\n",
    "except Exception as e:\n",
    "    print(f\"Streaming query failed: {str(e)}\", file=sys.stderr)\n",
    "    handle_shutdown(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13468dbe-9130-4395-93fb-7ec9a4d15206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
